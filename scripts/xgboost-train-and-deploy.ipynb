{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c288e80",
   "metadata": {},
   "source": [
    "# Train and Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300a702",
   "metadata": {},
   "source": [
    "Prepare data, train XGBoost model, and deploy as real time endpoint to sagemaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e2d27",
   "metadata": {},
   "source": [
    "## Data Preparation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a14d801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Education                     int8\n",
      "JoiningYear                  int64\n",
      "City                          int8\n",
      "PaymentTier                  int64\n",
      "Age                          int64\n",
      "Gender                        int8\n",
      "EverBenched                  int64\n",
      "ExperienceInCurrentDomain    int64\n",
      "LeaveOrNot                   int64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LeaveOrNot</th>\n",
       "      <th>Education</th>\n",
       "      <th>JoiningYear</th>\n",
       "      <th>City</th>\n",
       "      <th>PaymentTier</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>EverBenched</th>\n",
       "      <th>ExperienceInCurrentDomain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LeaveOrNot  Education  JoiningYear  City  PaymentTier  Age  Gender  \\\n",
       "0           0          0         2017     0            3   34       1   \n",
       "1           1          0         2013     2            1   28       0   \n",
       "2           0          0         2014     1            3   38       0   \n",
       "3           1          1         2016     0            3   27       1   \n",
       "4           1          1         2017     2            3   24       1   \n",
       "\n",
       "   EverBenched  ExperienceInCurrentDomain  \n",
       "0            0                          0  \n",
       "1            0                          3  \n",
       "2            0                          2  \n",
       "3            0                          5  \n",
       "4            1                          2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Setup\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for storing data\n",
    "bucket = 'kennys-testing-bucket'\n",
    "prefix = 'deployment'\n",
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Employee.csv'  # Replace with your actual file path in S3 if needed\n",
    "employee_df = pd.read_csv(file_path)\n",
    "employee_df.head()\n",
    "\n",
    "# Step 2: Data Preparation\n",
    "# Convert categorical columns to numeric\n",
    "employee_df['Education'] = employee_df['Education'].astype('category').cat.codes\n",
    "employee_df['City'] = employee_df['City'].astype('category').cat.codes\n",
    "employee_df['Gender'] = employee_df['Gender'].astype('category').cat.codes\n",
    "employee_df['EverBenched'] = employee_df['EverBenched'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Drop rows with NaN values in the target column\n",
    "employee_df.dropna(subset=['LeaveOrNot'])\n",
    "\n",
    "# Convert target column to numeric if needed\n",
    "employee_df['LeaveOrNot'] = employee_df['LeaveOrNot'].astype(int)\n",
    "\n",
    "# Ensure no missing values in feature columns\n",
    "employee_df = employee_df.dropna()\n",
    "\n",
    "# Verify all columns are numeric\n",
    "print(employee_df.dtypes)\n",
    "\n",
    "# Define features and target\n",
    "feature_columns = [\n",
    "    'Education', 'JoiningYear', 'City', 'PaymentTier', 'Age',\n",
    "    'Gender', 'EverBenched', 'ExperienceInCurrentDomain'\n",
    "]\n",
    "target_column = 'LeaveOrNot'\n",
    "\n",
    "employee_df = employee_df[[target_column] + feature_columns]\n",
    "\n",
    "train_df, test_df = train_test_split(employee_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the transformed dataset\n",
    "employee_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca50e267-39f2-45f3-bc39-b4065b6bf0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to s3://kennys-testing-bucket/deployment/train/train.csv\n",
      "Validation data uploaded to s3://kennys-testing-bucket/deployment/validation/validation.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-12-30-20-01-36-049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-30 20:01:38 Starting - Starting the training job...\n",
      "2024-12-30 20:01:52 Starting - Preparing the instances for training...\n",
      "2024-12-30 20:02:32 Downloading - Downloading the training image...\n",
      "2024-12-30 20:03:08 Training - Training image download completed. Training in progress...\u001b[34m[2024-12-30 20:03:18.074 ip-10-0-141-69.us-east-2.compute.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-12-30 20:03:18.101 ip-10-0-141-69.us-east-2.compute.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] Train matrix has 3723 rows and 8 columns\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] Validation matrix has 932 rows\u001b[0m\n",
      "\u001b[34m[2024-12-30 20:03:18.158 ip-10-0-141-69.us-east-2.compute.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-12-30 20:03:18.159 ip-10-0-141-69.us-east-2.compute.internal:7 INFO hook.py:207] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-12-30 20:03:18.159 ip-10-0-141-69.us-east-2.compute.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-12-30 20:03:18.159 ip-10-0-141-69.us-east-2.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-12-30:20:03:18:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[20:03:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\u001b[34m[2024-12-30 20:03:18.164 ip-10-0-141-69.us-east-2.compute.internal:7 INFO hook.py:428] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2024-12-30 20:03:18.167 ip-10-0-141-69.us-east-2.compute.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[0]#011train-logloss:0.61438#011validation-logloss:0.60751\u001b[0m\n",
      "\u001b[34m[1]#011train-logloss:0.55715#011validation-logloss:0.54619\u001b[0m\n",
      "\u001b[34m[2]#011train-logloss:0.51811#011validation-logloss:0.50401\u001b[0m\n",
      "\u001b[34m[3]#011train-logloss:0.49991#011validation-logloss:0.48395\u001b[0m\n",
      "\u001b[34m[4]#011train-logloss:0.48661#011validation-logloss:0.46905\u001b[0m\n",
      "\u001b[34m[5]#011train-logloss:0.46939#011validation-logloss:0.45047\u001b[0m\n",
      "\u001b[34m[6]#011train-logloss:0.46506#011validation-logloss:0.44522\u001b[0m\n",
      "\u001b[34m[7]#011train-logloss:0.45596#011validation-logloss:0.43293\u001b[0m\n",
      "\u001b[34m[8]#011train-logloss:0.45035#011validation-logloss:0.42719\u001b[0m\n",
      "\u001b[34m[9]#011train-logloss:0.44423#011validation-logloss:0.42068\u001b[0m\n",
      "\u001b[34m[10]#011train-logloss:0.44393#011validation-logloss:0.42025\u001b[0m\n",
      "\u001b[34m[11]#011train-logloss:0.44375#011validation-logloss:0.41998\u001b[0m\n",
      "\u001b[34m[12]#011train-logloss:0.44363#011validation-logloss:0.41976\u001b[0m\n",
      "\u001b[34m[13]#011train-logloss:0.44361#011validation-logloss:0.41970\u001b[0m\n",
      "\u001b[34m[14]#011train-logloss:0.44356#011validation-logloss:0.41959\u001b[0m\n",
      "\u001b[34m[15]#011train-logloss:0.44355#011validation-logloss:0.41955\u001b[0m\n",
      "\u001b[34m[16]#011train-logloss:0.44355#011validation-logloss:0.41956\u001b[0m\n",
      "\u001b[34m[17]#011train-logloss:0.43763#011validation-logloss:0.41146\u001b[0m\n",
      "\u001b[34m[18]#011train-logloss:0.43764#011validation-logloss:0.41148\u001b[0m\n",
      "\u001b[34m[19]#011train-logloss:0.43763#011validation-logloss:0.41146\u001b[0m\n",
      "\u001b[34m[20]#011train-logloss:0.43763#011validation-logloss:0.41145\u001b[0m\n",
      "\u001b[34m[21]#011train-logloss:0.43763#011validation-logloss:0.41146\u001b[0m\n",
      "\u001b[34m[22]#011train-logloss:0.43763#011validation-logloss:0.41146\u001b[0m\n",
      "\u001b[34m[23]#011train-logloss:0.43764#011validation-logloss:0.41148\u001b[0m\n",
      "\u001b[34m[24]#011train-logloss:0.43765#011validation-logloss:0.41151\u001b[0m\n",
      "\u001b[34m[25]#011train-logloss:0.43765#011validation-logloss:0.41151\u001b[0m\n",
      "\u001b[34m[26]#011train-logloss:0.43764#011validation-logloss:0.41148\u001b[0m\n",
      "\u001b[34m[27]#011train-logloss:0.43764#011validation-logloss:0.41149\u001b[0m\n",
      "\u001b[34m[28]#011train-logloss:0.43763#011validation-logloss:0.41145\u001b[0m\n",
      "\u001b[34m[29]#011train-logloss:0.43763#011validation-logloss:0.41144\u001b[0m\n",
      "\u001b[34m[30]#011train-logloss:0.43765#011validation-logloss:0.41151\u001b[0m\n",
      "\u001b[34m[31]#011train-logloss:0.43765#011validation-logloss:0.41152\u001b[0m\n",
      "\u001b[34m[32]#011train-logloss:0.43765#011validation-logloss:0.41153\u001b[0m\n",
      "\u001b[34m[33]#011train-logloss:0.43765#011validation-logloss:0.41152\u001b[0m\n",
      "\u001b[34m[34]#011train-logloss:0.43764#011validation-logloss:0.41151\u001b[0m\n",
      "\u001b[34m[35]#011train-logloss:0.43764#011validation-logloss:0.41149\u001b[0m\n",
      "\u001b[34m[36]#011train-logloss:0.43763#011validation-logloss:0.41145\u001b[0m\n",
      "\u001b[34m[37]#011train-logloss:0.43763#011validation-logloss:0.41145\u001b[0m\n",
      "\u001b[34m[38]#011train-logloss:0.43763#011validation-logloss:0.41143\u001b[0m\n",
      "\u001b[34m[39]#011train-logloss:0.43763#011validation-logloss:0.41141\u001b[0m\n",
      "\u001b[34m[40]#011train-logloss:0.43763#011validation-logloss:0.41144\u001b[0m\n",
      "\u001b[34m[41]#011train-logloss:0.43478#011validation-logloss:0.40798\u001b[0m\n",
      "\u001b[34m[42]#011train-logloss:0.43479#011validation-logloss:0.40803\u001b[0m\n",
      "\u001b[34m[43]#011train-logloss:0.43480#011validation-logloss:0.40804\u001b[0m\n",
      "\u001b[34m[44]#011train-logloss:0.43478#011validation-logloss:0.40800\u001b[0m\n",
      "\u001b[34m[45]#011train-logloss:0.43478#011validation-logloss:0.40799\u001b[0m\n",
      "\u001b[34m[46]#011train-logloss:0.43479#011validation-logloss:0.40801\u001b[0m\n",
      "\u001b[34m[47]#011train-logloss:0.43478#011validation-logloss:0.40799\u001b[0m\n",
      "\u001b[34m[48]#011train-logloss:0.43478#011validation-logloss:0.40799\u001b[0m\n",
      "\u001b[34m[49]#011train-logloss:0.43478#011validation-logloss:0.40797\u001b[0m\n",
      "\n",
      "2024-12-30 20:03:37 Uploading - Uploading generated training model\n",
      "2024-12-30 20:03:37 Completed - Training job completed\n",
      "Training seconds: 84\n",
      "Billable seconds: 84\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Save the data locally first\n",
    "train_file = 'train.csv'\n",
    "validation_file = 'validation.csv'\n",
    "train_df.to_csv(train_file, index=False)\n",
    "test_df.to_csv(validation_file, index=False)\n",
    "\n",
    "# Upload the data to S3\n",
    "s3.upload_file(train_file, bucket, f'{prefix}/train/{train_file}')\n",
    "s3.upload_file(validation_file, bucket, f'{prefix}/validation/{validation_file}')\n",
    "\n",
    "print(f\"Training data uploaded to s3://{bucket}/{prefix}/train/{train_file}\")\n",
    "print(f\"Validation data uploaded to s3://{bucket}/{prefix}/validation/{validation_file}\")\n",
    "\n",
    "# Setup XGBoost Estimator\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"1.3-1\")\n",
    "hyperparameters = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"eta\":\"0.2\",\n",
    "    \"gamma\":\"40\",\n",
    "    \"min_child_weight\":\"6\",\n",
    "    \"subsample\":\"0.7\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"num_round\":\"50\"\n",
    "}\n",
    "\n",
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgboost_container, \n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge', \n",
    "    volume_size=5,  # 5 GB \n",
    "    output_path=output_path\n",
    ")\n",
    "\n",
    "# Define the data type and paths to the training and validation datasets\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(f\"s3://{bucket}/{prefix}/train/{train_file}\", content_type=content_type)\n",
    "validation_input = TrainingInput(f\"s3://{bucket}/{prefix}/validation/{validation_file}\", content_type=content_type)\n",
    "\n",
    "# Execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e2420",
   "metadata": {},
   "source": [
    "## Deploy as Real-Time Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e52cfd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2024-12-30-20-04-35-095\n",
      "INFO:sagemaker:Creating endpoint-config with name xgboost-predictor-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name xgboost-predictor-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "\n",
    "# Deploy the model as a real-time endpoint\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,  # Number of instances to deploy\n",
    "    instance_type='ml.m5.xlarge',  # Instance type for the endpoint\n",
    "    endpoint_name='xgboost-predictor-realtime'  # Name of the endpoint\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4be099-9876-48d9-83b9-3baa5fda99ff",
   "metadata": {},
   "source": [
    "### Alternatively deploy from a model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be223b-9844-4915-b9b2-5c86bf3c5493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "# Specify the S3 path to the pre-trained model artifact\n",
    "model_artifact = \"s3://your-path/model.tar.gz\"\n",
    "\n",
    "# Retrieve the container image for the framework (e.g., XGBoost)\n",
    "container = sagemaker.image_uris.retrieve(framework=\"xgboost\", region=boto3.Session().region_name, version=\"1.3-1\")\n",
    "\n",
    "# Create the model object using the S3 path\n",
    "model = Model(\n",
    "    image_uri=container,\n",
    "    model_data=model_artifact,\n",
    "    role=sagemaker.get_execution_role()\n",
    ")\n",
    "\n",
    "# Deploy the model as a real-time endpoint\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,  # Number of instances\n",
    "    instance_type='ml.m5.xlarge',  # Instance type\n",
    "    endpoint_name='xgboost-predictor-realtime'  # Name of the endpoint\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994eb85-337f-4439-8163-2b007308e111",
   "metadata": {},
   "source": [
    "## Prepare data for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8dbf3fb-861d-4d5c-a1d4-40d89a5c270c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/kenne/AppData/Local/Microsoft/WindowsApps/python3.13.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optionally, configure the predictor for the specific input and output formats\n",
    "predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "# Example input data (in the same format as your training data)\n",
    "\n",
    "# Assuming 'LeaveOrNot' is the target column\n",
    "features_df = test_df.drop(columns=['LeaveOrNot'])\n",
    "\n",
    "# Convert the features DataFrame to CSV format\n",
    "test_csv = features_df.to_csv(index=False, header=False).strip()\n",
    "\n",
    "test_data = test_df[feature_columns].head(20)  # Select the first row of test data for prediction\n",
    "test_data_csv = test_data.to_csv(index=False, header=False).strip()  # Convert to CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344b14d-c27c-4d81-b339-c39ab6214995",
   "metadata": {},
   "source": [
    "## Invoke endpoint by finding the endpoint at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "321e40b9-29f4-486f-89c6-5d6832e4d224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20906561613082886\n",
      "0.1683102250099182\n",
      "0.38488397002220154\n",
      "0.4326663315296173\n",
      "0.15928953886032104\n",
      "0.46319764852523804\n",
      "0.6720890998840332\n",
      "0.9404475688934326\n",
      "0.15928953886032104\n",
      "0.9504724144935608\n",
      "0.12999935448169708\n",
      "0.20906561613082886\n",
      "0.9239230155944824\n",
      "0.12999935448169708\n",
      "0.2676047682762146\n",
      "0.9504724144935608\n",
      "0.15928953886032104\n",
      "0.42358148097991943\n",
      "0.47207778692245483\n",
      "0.12999935448169708\n",
      "0.15928953886032104\n",
      "0.5771342515945435\n",
      "0.709308922290802\n",
      "0.1683102250099182\n",
      "0.3065555989742279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the SageMaker runtime client\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Specify the endpoint name\n",
    "endpoint_name = 'xgboost-predictor-realtimer'\n",
    "\n",
    "# Invoke the endpoint directly using the runtime client\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",  # Specify the content type\n",
    "    Body=test_data_csv  # The input data as a CSV string\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
